{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import re\n",
    "import cPickle\n",
    "import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect path extensions for all restaurants included on the site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URL = 'https://www.viamichelin.com'\n",
    "global_extension = '/web/Restaurants?geoboundaries=\\\n",
    "-41.7713117,-156.09375:77.389504,128.3203125&page='\n",
    "\n",
    "paths = []\n",
    "start = datetime.datetime.now()\n",
    "print 'start time: %s\\n' %start\n",
    "\n",
    "for page in range(1, 779: \n",
    "    pre_mich = requests.get(URL + global_extension + str(page))\n",
    "    pre_soup = BeautifulSoup(pre_mich.content, \"lxml\")\n",
    "    for restaurant in pre_soup.find_all('a', href=re.compile('/web/Restaurant/')):\n",
    "        path = restaurant[\"href\"]\n",
    "        paths.append(path)\n",
    "    if page%10 == 0:\n",
    "        print round(float(page)/len(x), 2)\n",
    "\n",
    "print '\\ndone!'\n",
    "paths = pd.DataFrame(paths)\n",
    "paths.columns = ['path']\n",
    "paths.to_csv('assets/world_paths__CHANGE_PATH__.csv', index = False, encoding='utf-8')\n",
    "finish = datetime.datetime.now()\n",
    "print '\\nfinish time: %s' %finish\n",
    "print '\\ntime elapsed: %s' %(finish - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load path extensions from file (length of dataframe = number of restaurants):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "According to www.viamichelin.com, as of 08/09/2016 there are 18670 restaurants on the \n",
      "site; this list contains 18670.  We got 'em all!  Time to scrape.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paths = pd.read_csv('./assets/world_paths_KEEP.csv', index_col = False)\n",
    "\n",
    "diff = 18670 - len(paths)\n",
    "\n",
    "if diff == 0:\n",
    "    x = 'We got \\'em all!  Time to scrape.'\n",
    "else:\n",
    "    x = 'Uh oh, something happened...'\n",
    "\n",
    "print '\\n'\n",
    "print 'According to www.viamichelin.com, as of 08/09/2016 there are 18670 restaurants on the \\n\\\n",
    "site; this list contains %s.  ' %len(paths) + x + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first seven path extensions from dataframe 'paths', hand-pick three more -- one 3-star, one 2-star, and one bib-gourmand with no photos -- and get soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_paths = pd.DataFrame(paths[0:6]['path'])\n",
    "\n",
    "# ## include a 3-star, 2-star, bib gourmand in test paths\n",
    "# special_list = ['/web/Restaurant/MODENA-41121-Osteria_Francescana-141517-41102',\\\n",
    "#                 '/web/Restaurant/New_York-10013-Nom_Wah_Tea_Parlor-336505-41102',\\\n",
    "#                 '/web/Restaurant/New_York-10013-atera-366502-41102']\n",
    "\n",
    "# special_paths = pd.DataFrame(special_list, columns = ['path'])\n",
    "\n",
    "# test_paths = pd.concat([test_paths, special_paths])\n",
    "# test_paths = test_paths.reset_index(drop = True)\n",
    "            \n",
    "# URL = 'https://www.viamichelin.com'\n",
    "\n",
    "# test_soup = []\n",
    "\n",
    "# for i in range(len(test_paths)):\n",
    "#     time.sleep(1)\n",
    "#     tiny_request = requests.get(URL + test_paths['path'][i])\n",
    "#     if str(tiny_request) != '<Response [200]>':\n",
    "#         print URL + test_paths['path'][i]\n",
    "#     tiny_soup = BeautifulSoup(tiny_request.content, \"lxml\")\n",
    "#     test_soup.append(tiny_soup)\n",
    "\n",
    "# cPickle.dump(test_soup, open('test_soup.p', 'wb')) \n",
    "# print len(test_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of soups, 'test_soup,' from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_soup = cPickle.load(open('test_soup.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape every restaurant page on the site, save list of soups to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2016-08-16 12:06:22.350813\n",
      "0.003 0.056 0.109 0.163 0.216 0.269 0.322 0.376 0.429 0.482 0.536 0.589 0.642 0.696 \n",
      "\n",
      "https://www.viamichelin.com/web/Restaurant/Seaview-PO34_5EX-The_Restaurant-194215-41102\n",
      "\n",
      "\n",
      "0.749 0.802 0.856 0.909 0.962 \n",
      "\n",
      "all done!  that took 1:02:06.742247.  pickle time...\n",
      "\n",
      "finish time: 2016-08-16 13:10:49.500809\n",
      "\n",
      "elapsed time: 1:04:27.149996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = \n",
    "\n",
    "URL = 'https://www.viamichelin.com'\n",
    "world_soup = []\n",
    "\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print 'start time: %s' %start\n",
    "range_list = [\n",
    "    range(1, 1866),      #n = 0\n",
    "    range(1867, 5597),   #n = 1\n",
    "    range(5598, 7463),   #n = 2\n",
    "    range(7464, 9329),   #n = 3\n",
    "    range(9330, 11195),  #n = 4\n",
    "    range(11196, 13061), #n = 5\n",
    "    range(13062, 14927), #n = 6\n",
    "    range(14928, 16793), #n = 7\n",
    "    range(16794, 18670)  #n = 8\n",
    "    ]\n",
    "\n",
    "count = -1\n",
    "\n",
    "for i in range_list[n]: ## first part out of 18670\n",
    "    time.sleep(1)\n",
    "    tiny_request = requests.get(URL + paths['path'][i])\n",
    "    if str(tiny_request) != '<Response [200]>':\n",
    "        print '\\n'\n",
    "        print URL + paths['path'][i]\n",
    "        print '\\n'\n",
    "    tiny_soup = BeautifulSoup(tiny_request.content, \"lxml\")\n",
    "    world_soup.append(tiny_soup)\n",
    "    if i%100 == 0:\n",
    "        print round(float(count)/len(range_list[n]), 3),\n",
    "        os.system('say \"%s\"' %(float(count)/100))\n",
    "    count+= 1\n",
    "    \n",
    "midway = datetime.datetime.now()\n",
    "print '\\n\\nall done!  that took %s.  pickle time...' %(midway - start)\n",
    "\n",
    "name = 'world_soup_{0}of10.p'.format(n+1)\n",
    "\n",
    "cPickle.dump(world_soup, open(name, 'wb'))\n",
    "\n",
    "finish = datetime.datetime.now()\n",
    "print '\\nfinish time: %s' %finish\n",
    "print '\\nelapsed time: %s' %(finish - start)\n",
    "os.system('say \"Hey! I\\'m finally done with your fucking program\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "str(tiny_request) != '<Response [200]>':\n",
    "\n",
    "n = 0\n",
    "https://www.viamichelin.com/web/Restaurant/Bastelicaccia-20123-Auberge_du_Prunelli-2452-41102\n",
    "http://www.viamichelin.com/web/Restaurant/Pouilly_en_Auxois-21320-Restaurant_de_la_Poste-12494-41102\n",
    "\n",
    "n = 1 -- elapsed time: 1:52:54.186918 (double-long, though)\n",
    "https://www.viamichelin.com/web/Restaurant/Tessenderlo-3980-La_Forchetta-210463-41102\n",
    "https://www.viamichelin.com/web/Restaurant/CATTOLICA-47841-Locanda_Liuzzi-130338-41102\n",
    "\n",
    "n = 2 -- elapsed time: 0:59:13.456565\n",
    "\n",
    "n = 3 -- elapsed time: 1:00:09.302337\n",
    "\n",
    "n = 4 -- elapsed time: 0:59:54.953228\n",
    "\n",
    "n = 5 -- elapsed time: 1:05:12.635171\n",
    "\n",
    "n = 6 -- elapsed time: 1:04:22.481197\n",
    "\n",
    "n = 7\n",
    "https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102\n",
    "https://www.viamichelin.com/web/Restaurant/Geneve-1201-Le_Lexique-236454-41102\n",
    "\n",
    "n = 8\n",
    "https://www.viamichelin.com/web/Restaurant/Seaview-PO34_5EX-The_Restaurant-194215-41102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 9\n",
    "\n",
    "dontforgetus = [\n",
    "    'https://www.viamichelin.com/web/Restaurant/Bastelicaccia-20123-Auberge_du_Prunelli-2452-41102',\n",
    "    'http://www.viamichelin.com/web/Restaurant/Pouilly_en_Auxois-21320-Restaurant_de_la_Poste-12494-41102',\n",
    "    'https://www.viamichelin.com/web/Restaurant/Tessenderlo-3980-La_Forchetta-210463-41102',\n",
    "    'https://www.viamichelin.com/web/Restaurant/CATTOLICA-47841-Locanda_Liuzzi-130338-41102',\n",
    "    'https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102',\n",
    "    'https://www.viamichelin.com/web/Restaurant/Geneve-1201-Le_Lexique-236454-41102',\n",
    "    'https://www.viamichelin.com/web/Restaurant/Seaview-PO34_5EX-The_Restaurant-194215-41102'\n",
    "    ]\n",
    "\n",
    "world_soup = []\n",
    "\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print 'start time: %s' %start\n",
    "\n",
    "count = -1\n",
    "\n",
    "for i in dontforgetus: \n",
    "    time.sleep(1)\n",
    "    tiny_request = requests.get(i)\n",
    "    if str(tiny_request) != '<Response [200]>':\n",
    "        print '\\n'\n",
    "        print i\n",
    "        print '\\n'\n",
    "    tiny_soup = BeautifulSoup(tiny_request.content, \"lxml\")\n",
    "    world_soup.append(tiny_soup)\n",
    "    count+= 1\n",
    "    print count,\n",
    "    \n",
    "midway = datetime.datetime.now()\n",
    "print '\\n\\nall done!  that took %s.  pickle time...' %(midway - start)\n",
    "\n",
    "name = 'world_soup_{0}of10.p'.format(n+1)\n",
    "\n",
    "cPickle.dump(world_soup, open(name, 'wb'))\n",
    "\n",
    "finish = datetime.datetime.now()\n",
    "print '\\nfinish time: %s' %finish\n",
    "print '\\nelapsed time: %s' %(finish - start)\n",
    "os.system('say \"Hey! I\\'m finally done with your fucking program\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start time: 2016-08-16 14:33:41.760509\n",
    "0 1 2 3 \n",
    "\n",
    "https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102\n",
    "\n",
    "\n",
    "4 5 6 \n",
    "\n",
    "all done!  that took 0:00:12.352930.  pickle time...\n",
    "\n",
    "finish time: 2016-08-16 14:33:54.487622\n",
    "\n",
    "elapsed time: 0:00:12.727113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restaurantify(list_soup):\n",
    "    \n",
    "    '''This function creates a dataframe with restaurant observations and columns \\\n",
    "     containing relevant information.'''\n",
    "    \n",
    "    name = []\n",
    "    cuisine = []\n",
    "    blurb = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    distinctions = []\n",
    "    photos = []\n",
    "    photo_count = []\n",
    "    add_info = []\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    comments = []\n",
    "    num_comments = []\n",
    "    special_comment = []\n",
    "    currency = []\n",
    "    \n",
    "    count = -1\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print 'restaurantify start time: %s' %now\n",
    "\n",
    "    \n",
    "    for tiny_soup in list_soup:\n",
    "        \n",
    "        ## GET RESTAURANT NAME\n",
    "        name_result = tiny_soup.findAll('div', class_ = 'datasheet-item datasheet-name')\n",
    "        if name_result:\n",
    "            for rest_name in name_result:\n",
    "                name_text = rest_name.text.encode('utf-8')\n",
    "                name_text = name_text.replace('\\n\\t', '')\n",
    "                name_text = name_text.replace('\\n', '')\n",
    "                name.append(name_text)\n",
    "        else:\n",
    "            name.append(None)\n",
    "              \n",
    "        ## GET RESTAURANT CUISINE\n",
    "        cuisine_result = tiny_soup.findAll('div', class_ = 'datasheet-cooking-type')\n",
    "        if cuisine_result:\n",
    "            for rest_cuisine in cuisine_result:\n",
    "                lil_cuisine = rest_cuisine.text.encode('utf-8')\n",
    "                lil_cuisine = lil_cuisine.replace('Cuisine', '')\n",
    "                lil_cuisine = lil_cuisine.replace('cuisine', '')\n",
    "                cuisine.append(lil_cuisine)\n",
    "        else:\n",
    "            cuisine.append(None)\n",
    "                \n",
    "        ## GET BLURB\n",
    "        blurb_result = tiny_soup.findAll('div', class_ = 'datasheet-description')\n",
    "        if blurb_result:\n",
    "            for rest_blurb in blurb_result:\n",
    "                for j in rest_blurb.find_all('blockquote'):\n",
    "                    text = j.text.encode('utf-8')\n",
    "                    text = text.replace('\\n\\t\\t\\t', '')\n",
    "                    text = text.replace('\\t', '')\n",
    "                    blurb.append(text)\n",
    "        else:\n",
    "            blurb.append(None)\n",
    "            \n",
    "\n",
    "        ## GET PRICE RANGE\n",
    "        price_result = tiny_soup.findAll('div', class_ = 'datasheet-price')\n",
    "        if price_result:\n",
    "            for rest_price in price_result:\n",
    "                price_list = []\n",
    "                for j in rest_price.findAll('em'):\n",
    "                    price_list.append(j)\n",
    "                if len(price_list) == 2:\n",
    "                    low = price_list[0].text.encode('utf-8')\n",
    "                    low = low.split(' ')\n",
    "                    currency.append(low[1])\n",
    "                    try:\n",
    "                        low_price.append(int(low[0]))\n",
    "                    except:\n",
    "                        low_price.append(low[0])\n",
    "                    high = price_list[1].text.encode('utf-8')\n",
    "                    high = high.split(' ')\n",
    "                    try:\n",
    "                        high_price.append(int(high[0]))\n",
    "                    except:\n",
    "                        high_price.append(high[0])\n",
    "                else:\n",
    "                    currency.append(None)\n",
    "                    low_price.append(None)\n",
    "                    high_price.append(None)\n",
    "        else:\n",
    "            high_price.append(None)\n",
    "            low_price.append(None)\n",
    "            currency.append(None)\n",
    "            \n",
    "        \n",
    "        ## GET DISTINCTIONS\n",
    "        distinct_result = tiny_soup.findAll('div', class_ = 'datasheet-quotation')\n",
    "        if distinct_result:\n",
    "            for big_distinction in distinct_result:\n",
    "                distinction_list = []\n",
    "                for lil_distinction in big_distinction.findAll('span'):\n",
    "                    distinction_list.append(lil_distinction['class'])\n",
    "                distinctions.append(distinction_list)\n",
    "        else:\n",
    "            distinctions.append(None)\n",
    "            \n",
    "        \n",
    "        ## PHOTO COUNTS\n",
    "        photocount_result = tiny_soup.findAll('section', class_ = 'datasheet')\n",
    "        if photocount_result:\n",
    "            for big_mess in photocount_result:\n",
    "                mess = big_mess.find('span', class_ = 'icon-photo')\n",
    "                if mess:\n",
    "                    photo_count.append(int(mess.text))  # hasn't been run w 'int()' yet\n",
    "                else:\n",
    "                    photo_count.append(0)\n",
    "        else:\n",
    "            photo_count.append(0)\n",
    "        \n",
    "        \n",
    "        ## GET PHOTO URLs\n",
    "        photo_URLs = []\n",
    "        photo_result = tiny_soup.findAll('div', class_ = \\\n",
    "                                         'datasheet-more-info datasheet-photo clearfx')\n",
    "        if photo_result:\n",
    "            for urls in photo_result:\n",
    "                if urls:\n",
    "                    url_result = urls.findAll('img')\n",
    "                    if url_result:\n",
    "                        for url in url_result:\n",
    "                            lil_url = url['src']\n",
    "#                            lil_url = lil_url.replace('https://download.viamichelin.com', '')\n",
    "                            photo_URLs.append(lil_url)\n",
    "                        photos.append(photo_URLs)\n",
    "                    else:\n",
    "                        photos.append([])\n",
    "                else:\n",
    "                    photos.append([])\n",
    "        else:\n",
    "            photos.append([])\n",
    "\n",
    "            \n",
    "        ## GET MESSY ADDITIONAL INFO, HAVE FUN DEALING WITH IT LATER \n",
    "        infos = []\n",
    "        big_info = tiny_soup.findAll('div', class_ = 'datasheet-more-info clearfx')\n",
    "        if big_info:\n",
    "            for rest_add_info in big_info:\n",
    "                for advice in rest_add_info.findAll('li'):\n",
    "                    if advice:\n",
    "                        infos.append(advice.text.encode('utf-8'))\n",
    "                    else:\n",
    "                        infos.append(None)\n",
    "                        add_info.append(None) ## NEW\n",
    "            add_info.append(infos)\n",
    "        else:\n",
    "            add_info.append(None)\n",
    "        \n",
    "        \n",
    "        ## GET LATITUDE AND LONGITUDE \n",
    "        geolocs = tiny_soup.findAll('div', class_ = 'datasheet-link-to-itinerary')\n",
    "        if geolocs:\n",
    "            for lil_latlon in geolocs:\n",
    "                for link in lil_latlon.findAll('a'):\n",
    "                    latlon_split = link['href']\n",
    "                    latlon_split = (latlon_split.split(';'))[1]\n",
    "                    latlon_split = latlon_split.split(':')\n",
    "                    latitude.append(float(latlon_split[1]))\n",
    "                    longitude.append(float(latlon_split[0]))\n",
    "        else:\n",
    "            latitude.append(None)\n",
    "            longitude.append(None)\n",
    "            \n",
    "        \n",
    "        ## GET COMMENT LIST\n",
    "        comment_mess = tiny_soup.find_all(string=lambda text:isinstance(text, Comment))\n",
    "        if comment_mess:\n",
    "            comment_list = []\n",
    "            for comment in comment_mess:\n",
    "                x = comment.extract()\n",
    "                comment_list.append(x)\n",
    "            comments.append(comment_list)\n",
    "            num_comments.append(len(comment_list))\n",
    "            special_comment.append(comment_list[5])\n",
    "        else:\n",
    "            comments.append(None)  \n",
    "            num_comments.append(0)\n",
    "            special_comment.append(None)\n",
    "            \n",
    "            \n",
    "        ## MAKE SURE NO MISMATCHES IN LIST LENGTH    \n",
    "        length_list = [len(name), len(cuisine), len(blurb), len(high_price), len(low_price), \\\n",
    "                   len(distinctions), len(photos), len(photo_count), len(add_info),\\\n",
    "                  len(latitude), len(longitude), len(comments), len(num_comments), \\\n",
    "                   len(special_comment), len(currency)]\n",
    "        \n",
    "        if len(set(length_list)) > 1:\n",
    "            \n",
    "            print '\\n'\n",
    "            print set(length_list)\n",
    "            print 'Things got screwy near %s!\\n' %name_text\n",
    "            print 'name: %s' %len(name)\n",
    "            print 'cuisine: %s' %len(cuisine)\n",
    "            print 'distinctions: %s' %len(distinctions)\n",
    "            print 'blurb: %s' %len(blurb)\n",
    "            print 'high_price: %s' %len(high_price)\n",
    "            print 'low_price: %s' %len(low_price)  \n",
    "            print 'photos: %s' %len(photos)\n",
    "            print 'photo_count: %s' %len(photo_count)\n",
    "            print 'add_info: %s' %len(add_info)\n",
    "            print 'latitude: %s' %len(latitude)\n",
    "            print 'longitude: %s' %len(longitude)\n",
    "            print 'comments: %s' %len(comments)\n",
    "            print 'num_comments: %s' %len(num_comments)\n",
    "            print 'special_comment: %s' %len(special_comment)\n",
    "            os.system('say \"Uh oh, something fucked up.\"')\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        if count%100 == 0:\n",
    "            print float(count)/len(list_soup),\n",
    "            print 'line: %s' %count,\n",
    "            \n",
    "    ## GET WORD COUNT FOR BLURB\n",
    "    count2 = 0\n",
    "    num_words = []\n",
    "    print '\\n'\n",
    "    for blurby in blurb:\n",
    "        if blurby:\n",
    "            x = blurby.split(' ')\n",
    "            countx = len(x)\n",
    "            num_words.append(countx)\n",
    "        else:\n",
    "            num_words.append(0)\n",
    "        if float(count2)%1000 == 0:\n",
    "            print count2\n",
    "            count2 += 1\n",
    "\n",
    "    print '\\n'\n",
    "    print 'name: %s' %len(name)\n",
    "    print 'cuisine: %s' %len(cuisine)\n",
    "    print 'distinctions: %s' %len(distinctions)\n",
    "    print 'blurb: %s' %len(blurb)\n",
    "    print 'high_price: %s' %len(high_price)\n",
    "    print 'low_price: %s' %len(low_price)  \n",
    "    print 'photos: %s' %len(photos)\n",
    "    print 'photo_count: %s' %len(photo_count)\n",
    "    print 'add_info: %s' %len(add_info)\n",
    "    print 'latitude: %s' %len(latitude)\n",
    "    print 'longitude: %s' %len(longitude)\n",
    "    print 'comments: %s' %len(comments)\n",
    "    print 'num_comments: %s' %len(num_comments)\n",
    "    print 'special_comment: %s' %len(special_comment)\n",
    "    print 'num_words: %s' %len(num_words)\n",
    "    \n",
    "    df = pd.DataFrame(zip(name, cuisine, blurb, num_words, high_price, low_price, currency, \\\n",
    "distinctions, photos, photo_count, add_info, latitude, longitude, comments, num_comments, \\\n",
    "special_comment), columns = ['name', 'cuisine', 'blurb', 'num_words', 'high_price', \\\n",
    "'low_price', 'currency', 'distinctions', 'photos', 'photo_count', 'add_info', 'latitude', \\\n",
    "'longitude', 'comments', 'num_comments', 'special_comment'])\n",
    "    \n",
    "    print '\\n'\n",
    "    now_now = datetime.datetime.now()\n",
    "    print 'restaurantify end time: %s' %now_now\n",
    "    elapsed_now = now_now - now\n",
    "    print 'restaurantify elapsed time: %s' %elapsed_now\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurantify start time: 2016-08-16 14:35:25.937100\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "name: 7\n",
      "cuisine: 7\n",
      "distinctions: 7\n",
      "blurb: 7\n",
      "high_price: 7\n",
      "low_price: 7\n",
      "photos: 7\n",
      "photo_count: 7\n",
      "add_info: 7\n",
      "latitude: 7\n",
      "longitude: 7\n",
      "comments: 7\n",
      "num_comments: 7\n",
      "special_comment: 7\n",
      "num_words: 7\n",
      "\n",
      "\n",
      "restaurantify end time: 2016-08-16 14:35:26.847823\n",
      "restaurantify elapsed time: 0:00:00.910723\n",
      "\n",
      " pickle start time: 2016-08-16 14:35:25.937051\n",
      "whole shebang done at: 0:00:00.932934\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "lil_df = restaurantify(world_soup)\n",
    "print '\\n pickle start time: %s' %start\n",
    "cPickle.dump(lil_df, open('df_{0}of10.p'.format(n+1), 'wb')) \n",
    "finish = datetime.datetime.now()\n",
    "print 'whole shebang done at: %s' %(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1of10 = cPickle.load(open('df_1of10.p', 'rb'))\n",
    "df_2of10 = cPickle.load(open('df_2of10.p', 'rb'))\n",
    "df_3of10 = cPickle.load(open('df_3of10.p', 'rb'))\n",
    "df_4of10 = cPickle.load(open('df_4of10.p', 'rb'))\n",
    "df_5of10 = cPickle.load(open('df_5of10.p', 'rb'))\n",
    "df_6of10 = cPickle.load(open('df_6of10.p', 'rb'))\n",
    "df_7of10 = cPickle.load(open('df_7of10.p', 'rb'))\n",
    "df_8of10 = cPickle.load(open('df_8of10.p', 'rb'))\n",
    "df_9of10 = cPickle.load(open('df_9of10.p', 'rb'))\n",
    "df_10of10 = cPickle.load(open('df_10of10.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_1of10, df_2of10, df_3of10, df_4of10, df_5of10, df_6of10, df_7of10, \\\n",
    "                df_8of10, df_9of10, df_10of10], axis = 0)\n",
    "cPickle.dump(df, open('big_df.p', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18668"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restaurantify test_soup, save to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurantify start time: 2016-08-16 14:41:21.994128\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "name: 9\n",
      "cuisine: 9\n",
      "distinctions: 9\n",
      "blurb: 9\n",
      "high_price: 9\n",
      "low_price: 9\n",
      "photos: 9\n",
      "photo_count: 9\n",
      "add_info: 9\n",
      "latitude: 9\n",
      "longitude: 9\n",
      "comments: 9\n",
      "num_comments: 9\n",
      "special_comment: 9\n",
      "num_words: 9\n",
      "\n",
      "\n",
      "restaurantify end time: 2016-08-16 14:41:23.172331\n",
      "restaurantify elapsed time: 0:00:01.178203\n"
     ]
    }
   ],
   "source": [
    "test_soup = cPickle.load(open('test_soup.p', 'rb'))\n",
    "df_test = restaurantify(test_soup)\n",
    "cPickle.dump(df_test, open('df_test.p', 'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Good to go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
