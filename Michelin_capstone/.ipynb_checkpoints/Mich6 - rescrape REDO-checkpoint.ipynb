{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import cPickle\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load same list of path extensions used for main scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = pd.read_csv('assets/world_paths_KEEP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the scrape as many times as needed to finish all sites ('n' integer in [0, <#scrapes required>]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2016-08-24 13:23:05.050340\n",
      "\n",
      "\n",
      "\n",
      "scraping done!  that took 0:02:34.204605.  pickle time...\n",
      "\n",
      "finish time: 2016-08-24 13:25:39.255256\n",
      "\n",
      "elapsed time: 0:02:34.204916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 39\n",
    "\n",
    "URL = 'https://www.viamichelin.com'\n",
    "world_soup = []\n",
    "\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print 'start time: %s\\n' %start\n",
    "range_list = [  \n",
    "    range(1, 384),       #world_soup_1-1of10_chromedriver.p n = 0\n",
    "    range(384, 480),     #world_soup_1-2of10_chromedriver.p n = 1\n",
    "    range(480, 1268),    #world_soup_1-3of10_chromedriver.p n = 2\n",
    "    range(1268, 1324),   #world_soup_1-4of10_chromedriver.p n = 3\n",
    "    range(1324, 1565),   #world_soup_1-5of10_chromedriver.p n = 4\n",
    "    range(1565, 1867),   #world_soup_1-6of10_chromedriver.p n = 5\n",
    "    range(1867, 4335),   #world_soup_2-1of10_chromedriver.p n = 6\n",
    "    range(4335, 4556),   #world_soup_2-2of10_chromedriver.p n = 7\n",
    "    range(4556, 4958),   #world_soup_2-3of10_chromedriver.p n = 8\n",
    "    range(4958, 7463),   #world_soup_2-4of10_chromedriver.p n = 9\n",
    "    range(7463, 8204),   #world_soup_3-1of10_chromedriver.p n = 10\n",
    "    range(8204, 8798),   #world_soup_3-2of10_chromedriver.p n = 11\n",
    "    range(8798, 8882),   #world_soup_3-3of10_chromedriver.p n = 12\n",
    "    range(8882, 8922),   #world_soup_3-4of10_chromedriver.p n = 13\n",
    "    range(8922, 8959),   #world_soup_3-5of10_chromedriver.p n = 14\n",
    "    range(8959, 8996),   #world_soup_3-6of10_chromedriver.p n = 15\n",
    "    range(8996, 9330),   #world_soup_3-7of10_chromedriver.p n = 16\n",
    "    range(9330, 9633),   #world_soup_4-1of10_chromedriver.p n = 17\n",
    "    range(9633, 10525),  #world_soup_4-2of10_chromedriver.p n = 18\n",
    "    range(10525, 11196), #world_soup_4-3of10_chromedriver.p n = 19\n",
    "    range(11196, 12143), #world_soup_4-4of10_chromedriver.p n = 20\n",
    "    range(12143, 13061), #world_soup_4-5of10_chromedriver.p n = 21\n",
    "    range(13061, 13437), #world_soup_5-1of10_chromedriver.p n = 22\n",
    "    range(13437, 13523), #world_soup_5-2of10_chromedriver.p n = 23\n",
    "    range(13523, 13860), #world_soup_5-3of10_chromedriver.p n = 24\n",
    "    range(13860, 14377), #world_soup_5-4of10_chromedriver.p n = 25\n",
    "    range(14377, 14598), #world_soup_5-5of10_chromedriver.p n = 26\n",
    "    range(14598, 14928), #world_soup_5-6of10_chromedriver.p n = 27\n",
    "    range(14928, 15293), #world_soup_6-1of10_chromedriver.p n = 28\n",
    "    range(15293, 16228), #world_soup_6-2of10_chromedriver.p n = 29\n",
    "    range(16228, 16745), #world_soup_6-3of10_chromedriver.p n = 30\n",
    "    range(16745, 17262), #world_soup_6-4of10_chromedriver.p n = 31\n",
    "    range(17262, 17420), #world_soup_6-5of10_chromedriver.p n = 32   \n",
    "    range(17420, 18091), #world_soup_6-6of10_chromedriver.p n = 33 \n",
    "    range(18091, 18184), #world_soup_6-7of10_chromedriver.p n = 34 \n",
    "    range(18184, 18262), #world_soup_6-8of10_chromedriver.p n = 35 \n",
    "    range(18262, 18330), #world_soup_6-9of10_chromedriver.p n = 36\n",
    "    range(18330, 18357), #world_soup_6-10of10_chromedriver.p n = 37\n",
    "    range(18357, 18652), #world_soup_6-11of10_chromedriver.p n = 38 \n",
    "    range(18652, 18670)  #world_soup_6-12of10_chromedriver.p n = 39 \n",
    "    ]\n",
    "\n",
    "count = -1\n",
    "\n",
    "for i in range_list[n]: ## first part out of 18670\n",
    "    time.sleep(1)\n",
    "    \n",
    "    tiny_request = requests.get(URL + paths['path'][i])\n",
    "    if str(tiny_request) != '<Response [200]>':\n",
    "        print '\\n'\n",
    "        print URL + paths['path'][i]\n",
    "        print '\\n'\n",
    "    \n",
    "    chromedriver = '/Users/Rebecca/DSI-projects/projects/capstone/chromedriver'\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL + paths['path'][i])\n",
    "    world_soup.append(driver.page_source)\n",
    "    driver.close()\n",
    "          \n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print len(world_soup),\n",
    "        os.system('say \"%s\"' %(float(count)/len(range_list[n])))\n",
    "    count+= 1\n",
    "driver.quit()    \n",
    "    \n",
    "    \n",
    "midway = datetime.datetime.now()\n",
    "print '\\n\\nscraping done!  that took %s.  pickle time...' %(midway - start)\n",
    "\n",
    "name = 'world_soup_{0}of10_chromedriver.p'.format(n+1)\n",
    "\n",
    "#cPickle.dump(world_soup, open(name, 'wb'))\n",
    "\n",
    "finish = datetime.datetime.now()\n",
    "print '\\nfinish time: %s' %finish\n",
    "print '\\nelapsed time: %s' %(finish - start)\n",
    "os.system('say \"All done!\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of URLs with non-[200] response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.viamichelin.com/web/Restaurant/BOLZONE-26010-Trattoria_Via_Vai-148061-41102\n",
    "\n",
    "https://www.viamichelin.com/web/Restaurant/Muhlbach_sur_Munster-68380-Perle_des_Vosges-12021-41102\n",
    "\n",
    "https://www.viamichelin.com/web/Restaurant/Chicago-60602-Atwood-268972-41102\n",
    "\n",
    "https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final run for URLs in spat-out list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2016-08-25 01:04:22.168181\n",
      "\n",
      "\n",
      "\n",
      "https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scraping done!  that took 0:00:30.234236.  pickle time...\n",
      "\n",
      "finish time: 2016-08-25 01:04:52.413917\n",
      "\n",
      "elapsed time: 0:00:30.245736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dontforgetus = [\n",
    "'https://www.viamichelin.com/web/Restaurant/BOLZONE-26010-Trattoria_Via_Vai-148061-41102',\n",
    "'https://www.viamichelin.com/web/Restaurant/Muhlbach_sur_Munster-68380-Perle_des_Vosges\\\n",
    "                                                                                -12021-41102',\n",
    "'https://www.viamichelin.com/web/Restaurant/Chicago-60602-Atwood-268972-41102',\n",
    "'https://www.viamichelin.com/web/Restaurant/Biarritz-64200-Chez_Ospi-337162-41102']\n",
    "\n",
    "forgetful_soup = []\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print 'start time: %s\\n' %start\n",
    "\n",
    "for i in dontforgetus:\n",
    "    time.sleep(1)\n",
    "    \n",
    "    tiny_request = requests.get(i)\n",
    "    if str(tiny_request) != '<Response [200]>':\n",
    "        print '\\n'\n",
    "        print i\n",
    "        print '\\n'\n",
    "    \n",
    "    chromedriver = '/Users/Rebecca/DSI-projects/projects/capstone/chromedriver'\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(i)\n",
    "    forgetful_soup.append(driver.page_source)\n",
    "    driver.close()\n",
    "\n",
    "driver.quit()    \n",
    "    \n",
    "    \n",
    "midway = datetime.datetime.now()\n",
    "print '\\n\\nscraping done!  that took %s.  pickle time...' %(midway - start)\n",
    "\n",
    "cPickle.dump(forgetful_soup, open('world_soup_6-13of10_chromedriver.p', 'wb'))\n",
    "\n",
    "finish = datetime.datetime.now()\n",
    "print '\\nfinish time: %s' %finish\n",
    "print '\\nelapsed time: %s' %(finish - start)\n",
    "os.system('say \"All done!\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biarritz restaurant is a dead link; ignore it as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to pull out the 'script' tags, append them to a list (one per observation), and in the same row have some distinguising feature so we can merge dataframes afterward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inspectorize(list_soup):\n",
    "    \n",
    "    '''This function creates a dataframe with information about inspectors.'''\n",
    "    \n",
    "    name = []\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    scripts = []\n",
    "    \n",
    "    count = -1\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    #print 'inspectorize start time: %s' %now\n",
    "\n",
    "    \n",
    "    for tiny_soup in list_soup:\n",
    "        \n",
    "        ## GET RESTAURANT NAME\n",
    "        name_result = tiny_soup.findAll('div', class_ = 'datasheet-item datasheet-name')\n",
    "        if name_result:\n",
    "            for rest_name in name_result:\n",
    "                name_text = rest_name.text.encode('utf-8')\n",
    "                name_text = name_text.replace('\\n\\t', '')\n",
    "                name_text = name_text.replace('\\n', '')\n",
    "                name.append(name_text)\n",
    "        else:\n",
    "            name.append(None)\n",
    "            \n",
    "            \n",
    "        ## GET SCRIPTS\n",
    "        script_result = tiny_soup.findAll('script')\n",
    "        if script_result:\n",
    "            big_scripts = []\n",
    "            for lil_script in script_result:\n",
    "                scripty = lil_script.encode('utf-8')\n",
    "                big_scripts.append(scripty)\n",
    "            scripts.append(big_scripts)\n",
    "        else:\n",
    "            name.append(None)\n",
    "\n",
    "            \n",
    "        ## GET LATITUDE AND LONGITUDE \n",
    "        geolocs = tiny_soup.findAll('div', class_ = 'datasheet-link-to-itinerary')\n",
    "        if geolocs:\n",
    "            for lil_latlon in geolocs:\n",
    "                for link in lil_latlon.findAll('a'):\n",
    "                    latlon_split = link['href']\n",
    "                    latlon_split = (latlon_split.split(';'))[1]\n",
    "                    latlon_split = latlon_split.split(':')\n",
    "                    latitude.append(float(latlon_split[1]))\n",
    "                    longitude.append(float(latlon_split[0]))\n",
    "        else:\n",
    "            latitude.append(None)\n",
    "            longitude.append(None)\n",
    "            \n",
    "          \n",
    "        ## MAKE SURE NO MISMATCHES IN LIST LENGTH    \n",
    "        length_list = [len(name), len(scripts), len(latitude), len(longitude)]\n",
    "        \n",
    "        if len(set(length_list)) > 1:\n",
    "\n",
    "            print set(length_list)\n",
    "            print 'Things got screwy near %s!\\n' %name_text\n",
    "            print 'name: %s' %len(name)\n",
    "            print 'latitude: %s' %len(latitude)\n",
    "            print 'longitude: %s' %len(longitude)\n",
    "            print 'scripts: %s' %len(scripts)\n",
    "            os.system('say \"Problem!  Please check.\"')\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        count += 1\n",
    "        if count%100 == 0:\n",
    "            print float(count)/len(list_soup),\n",
    "  \n",
    "    df_scripts = pd.DataFrame(zip(name, latitude, longitude, scripts), \\\n",
    "        columns = ['name', 'latitude', 'longitude', 'scripts'])\n",
    "    \n",
    "    print '.',\n",
    "    now_now = datetime.datetime.now()\n",
    "    #print 'inspectorize end time: %s' %now_now\n",
    "    elapsed_now = now_now - now\n",
    "    #print 'inspectorize elapsed time: %s' %elapsed_now\n",
    "    \n",
    "    return df_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2016-08-25 16:33:57.815482\n",
      "soup1 took: 0:00:32.680873\n",
      "soup2 took: 0:01:12.471459\n",
      "soup3 took: 0:00:08.527393\n",
      "soup4 took: 0:00:05.506540\n",
      "soup5 took: 0:00:21.017000\n",
      "soup6 took: 0:00:25.793964\n",
      "soup7 took: 0:03:32.817353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0883ccb1779b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mstart7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'soup7 took: %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart7\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msoup_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/chromedriver scrapes/world_soup_2-2of10_chromedriver.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mstart8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'soup8 took: %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart8\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start0 = datetime.datetime.now()\n",
    "print 'start time: %s' %start0\n",
    "soup_1 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-1of10_chromedriver.p', 'rb'))\n",
    "start1 = datetime.datetime.now()\n",
    "print 'soup1 took: %s' %(start1 - start0)\n",
    "soup_2 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-2of10_chromedriver.p', 'rb'))\n",
    "start2 = datetime.datetime.now()\n",
    "print 'soup2 took: %s' %(start2 - start1)\n",
    "soup_3 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-3of10_chromedriver.p', 'rb'))\n",
    "start3 = datetime.datetime.now()\n",
    "print 'soup3 took: %s' %(start3 - start2)\n",
    "soup_4 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-4of10_chromedriver.p', 'rb'))\n",
    "start4 = datetime.datetime.now()\n",
    "print 'soup4 took: %s' %(start4 - start3)\n",
    "soup_5 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-5of10_chromedriver.p', 'rb'))\n",
    "start5 = datetime.datetime.now()\n",
    "print 'soup5 took: %s' %(start5 - start4)\n",
    "soup_6 = cPickle.load(open('assets/chromedriver scrapes/world_soup_1-6of10_chromedriver.p', 'rb'))\n",
    "start6 = datetime.datetime.now()\n",
    "print 'soup6 took: %s' %(start6 - start5)\n",
    "soup_7 = cPickle.load(open('assets/chromedriver scrapes/world_soup_2-1of10_chromedriver.p', 'rb'))\n",
    "start7 = datetime.datetime.now()\n",
    "print 'soup7 took: %s' %(start7 - start6)\n",
    "soup_8 = cPickle.load(open('assets/chromedriver scrapes/world_soup_2-2of10_chromedriver.p', 'rb'))\n",
    "start8 = datetime.datetime.now()\n",
    "print 'soup8 took: %s' %(start8 - start7)\n",
    "soup_9 = cPickle.load(open('assets/chromedriver scrapes/world_soup_2-3of10_chromedriver.p', 'rb'))\n",
    "start9 = datetime.datetime.now()\n",
    "print 'soup9 took: %s' %(start9 - start8)\n",
    "soup_10 = cPickle.load(open('assets/chromedriver scrapes/world_soup_2-4of10_chromedriver.p', 'rb'))\n",
    "start10 = datetime.datetime.now()\n",
    "print 'soup10 took: %s' %(start10 - start9)\n",
    "\n",
    "world_soups_1 = [soup_1, soup_2, soup_3, soup_4, soup_5, soup_6, soup_7, soup_8, soup_9, soup_10]\n",
    "\n",
    "biggest_df = pd.DataFrame()\n",
    "for i in [soup_1, soup_2]:\n",
    "    soup_list = []\n",
    "    print '.',\n",
    "    for j in i:\n",
    "        souplet = BeautifulSoup(j, 'lxml')\n",
    "        soup_list.append(souplet)\n",
    "    big_df = inspectorize(soup_list)\n",
    "    biggest_df = pd.concat([biggest_df, big_df], axis = 0)\n",
    "\n",
    "len(biggest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start0 = datetime.datetime.now()\n",
    "print 'start time: %s' %start0\n",
    "soup_11 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-1of10_chromedriver.p', 'rb'))\n",
    "start11 = datetime.datetime.now()\n",
    "print 'soup_11 took: %s' %(start11 - start0)\n",
    "soup_12 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-2of10_chromedriver.p', 'rb'))\n",
    "start12 = datetime.datetime.now()\n",
    "print 'soup_12 took: %s' %(start12 - start11)\n",
    "soup_13 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-3of10_chromedriver.p', 'rb'))\n",
    "start13 = datetime.datetime.now()\n",
    "print 'soup_13 took: %s' %(start13 - start12)\n",
    "soup_14 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-4of10_chromedriver.p', 'rb'))\n",
    "start14 = datetime.datetime.now()\n",
    "print 'soup_14 took: %s' %(start14 - start13)\n",
    "soup_15 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-5of10_chromedriver.p', 'rb'))\n",
    "start15 = datetime.datetime.now()\n",
    "print 'soup_15 took: %s' %(start15 - start14)\n",
    "soup_16 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-6of10_chromedriver.p', 'rb'))\n",
    "start16 = datetime.datetime.now()\n",
    "print 'soup_16 took: %s' %(start16 - start15)\n",
    "soup_17 = cPickle.load(open('assets/chromedriver scrapes/world_soup_3-7of10_chromedriver.p', 'rb'))\n",
    "start17 = datetime.datetime.now()\n",
    "print 'soup_17 took: %s' %(start17 - start16)\n",
    "soup_18 = cPickle.load(open('assets/chromedriver scrapes/world_soup_4-1of10_chromedriver.p', 'rb'))\n",
    "start18 = datetime.datetime.now()\n",
    "print 'soup_18 took: %s' %(start18 - start17)\n",
    "soup_19= cPickle.load(open('assets/chromedriver scrapes/world_soup_4-2of10_chromedriver.p', 'rb'))\n",
    "start19 = datetime.datetime.now()\n",
    "print 'soup_19 took: %s' %(start19 - start18)\n",
    "soup_20 = cPickle.load(open('assets/chromedriver scrapes/world_soup_4-3of10_chromedriver.p', 'rb'))\n",
    "start20 = datetime.datetime.now()\n",
    "print 'soup_20 took: %s' %(start20 - start19)\n",
    "soup_21 = cPickle.load(open('assets/chromedriver scrapes/world_soup_4-4of10_chromedriver.p', 'rb'))\n",
    "start21 = datetime.datetime.now()\n",
    "print 'soup_21 took: %s' %(start21 - start20)\n",
    "soup_22 = cPickle.load(open('assets/chromedriver scrapes/world_soup_4-5of10_chromedriver.p', 'rb'))\n",
    "start22 = datetime.datetime.now()\n",
    "print 'soup_22 took: %s' %(start22 - start21)\n",
    "\n",
    "world_soup_2 = [soup_11, soup_12, soup_13, soup_14, soup_15, soup_16, soup_17, soup_18, \\\n",
    "                soup_19, soup_20, soup_21, soup_22]\n",
    "\n",
    "for i in world_soup_2:\n",
    "    soup_list = []\n",
    "    print '.',\n",
    "    for j in i:\n",
    "        souplet = BeautifulSoup(j, 'lxml')\n",
    "        soup_list.append(souplet)\n",
    "    big_df = inspectorize(soup_list)\n",
    "    biggest_df = pd.concat([biggest_df, big_df], axis = 0)\n",
    "\n",
    "len(biggest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start0 = datetime.datetime.now()\n",
    "soup_23 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-1of10_chromedriver.p', 'rb'))\n",
    "start23 = datetime.datetime.now()\n",
    "print 'soup_23 took: %s' %(start23 - start0)\n",
    "soup_24 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-2of10_chromedriver.p', 'rb'))\n",
    "start24 = datetime.datetime.now()\n",
    "print 'soup_24 took: %s' %(start24 - start23)\n",
    "soup_25 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-3of10_chromedriver.p', 'rb'))\n",
    "start25 = datetime.datetime.now()\n",
    "print 'soup_25 took: %s' %(start25 - start24)\n",
    "soup_26 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-4of10_chromedriver.p', 'rb'))\n",
    "start26 = datetime.datetime.now()\n",
    "print 'soup_26 took: %s' %(start26 - start25)\n",
    "soup_27 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-5of10_chromedriver.p', 'rb'))\n",
    "start27 = datetime.datetime.now()\n",
    "print 'soup_27 took: %s' %(start27 - start26)\n",
    "soup_28 = cPickle.load(open('assets/chromedriver scrapes/world_soup_5-6of10_chromedriver.p', 'rb'))\n",
    "start28 = datetime.datetime.now()\n",
    "print 'soup_28 took: %s' %(start28 - start27)\n",
    "soup_29 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-12of10_chromedriver.p', 'rb'))\n",
    "start29 = datetime.datetime.now()\n",
    "print 'soup_29 took: %s' %(start29 - start28)\n",
    "soup_30 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-2of10_chromedriver.p', 'rb'))\n",
    "start30 = datetime.datetime.now()\n",
    "print 'soup_30 took: %s' %(start30 - start29)\n",
    "soup_31 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-3of10_chromedriver.p', 'rb'))\n",
    "start31 = datetime.datetime.now()\n",
    "print 'soup_31 took: %s' %(start31 - start30)\n",
    "soup_32 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-4of10_chromedriver.p', 'rb'))\n",
    "start32 = datetime.datetime.now()\n",
    "print 'soup_32 took: %s' %(start32 - start31)\n",
    "soup_33 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-5of10_chromedriver.p', 'rb'))\n",
    "start33 = datetime.datetime.now()\n",
    "print 'soup_33 took: %s' %(start33 - start32)\n",
    "soup_34 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-6of10_chromedriver.p', 'rb'))\n",
    "start34 = datetime.datetime.now()\n",
    "print 'soup_34 took: %s' %(start34 - start33)\n",
    "soup_35 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-7of10_chromedriver.p', 'rb'))\n",
    "start35 = datetime.datetime.now()\n",
    "print 'soup_35 took: %s' %(start35 - start34)\n",
    "soup_36 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-8of10_chromedriver.p', 'rb'))\n",
    "start36 = datetime.datetime.now()\n",
    "print 'soup_36 took: %s' %(start36 - start35)\n",
    "soup_37 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-9of10_chromedriver.p', 'rb'))\n",
    "start37 = datetime.datetime.now()\n",
    "print 'soup_37 took: %s' %(start37 - start36)\n",
    "soup_38 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-10of10_chromedriver.p', 'rb'))\n",
    "start38 = datetime.datetime.now()\n",
    "print 'soup_38 took: %s' %(start38 - start37)\n",
    "soup_39 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-11of10_chromedriver.p', 'rb'))\n",
    "start39 = datetime.datetime.now()\n",
    "print 'soup_39 took: %s' %(start39 - start38)\n",
    "soup_40 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-12of10_chromedriver.p', 'rb'))\n",
    "start40 = datetime.datetime.now()\n",
    "print 'soup_40 took: %s' %(start40 - start39)\n",
    "soup_41 = cPickle.load(open('assets/chromedriver scrapes/world_soup_6-13of10_chromedriver.p', 'rb'))\n",
    "start41 = datetime.datetime.now()\n",
    "print 'soup_41 took: %s' %(start41 - start40)\n",
    "\n",
    "world_soup_3 = [soup_23, soup_24, soup_25, soup_26, soup_27, soup_28, soup_29, soup_30, \\\n",
    "               soup_31, soup_32, soup_33, soup_34, soup_35, soup_36, soup_37, soup_38, \\\n",
    "               soup_39, soup_40, soup_41]\n",
    "\n",
    "count = -1\n",
    "for i in world_soup_3:\n",
    "    soup_list = []\n",
    "    count += 1\n",
    "    if count > 0: \n",
    "        print '.',\n",
    "    for j in i:\n",
    "        souplet = BeautifulSoup(j, 'lxml')\n",
    "        soup_list.append(souplet)\n",
    "    big_df = inspectorize(soup_list)\n",
    "    biggest_df = pd.concat([biggest_df, big_df], axis = 0)\n",
    "\n",
    "len(biggest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cPickle.dump(biggest_df, open('assets/df_JSON_blobs_unmerged.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonspot.com/en/json-encoding-and-decoding-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "(None, <function _new_Index at 0x1063c97d0>, (<class 'pandas.indexes.base.Index'>, {'data': array(['latitude', 'longitude'], dtype=object), 'name': None}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-de6e5a4727a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbiggest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/df_JSON_blobs_unmerged.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbiggest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiggest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlil_listy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiggest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlil_listy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiggest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scripts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \"\"\" This is called upon unpickling, rather than the default which doesn't\n\u001b[1;32m     68\u001b[0m     \u001b[0mhave\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbreaks\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: (None, <function _new_Index at 0x1063c97d0>, (<class 'pandas.indexes.base.Index'>, {'data': array(['latitude', 'longitude'], dtype=object), 'name': None}))"
     ]
    }
   ],
   "source": [
    "biggest_df = cPickle.load(open('assets/df_JSON_blobs_unmerged.p', 'rb'))\n",
    "biggest_df = biggest_df.reset_index(drop = True)\n",
    "lil_listy = []\n",
    "for i in range(len(biggest_df)):\n",
    "    lil_listy.append(len(biggest_df['scripts'][i]))\n",
    "set(lil_listy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12067"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biggest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18659"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_df = cPickle.load(open('assets/big_df_model-ready.p', 'rb'))\n",
    "len(old_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18742"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = old_df.merge(biggest_df, on=['name', 'latitude'], how = 'left')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18652"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['blurb'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['name', 'latitude']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['name', 'latitude'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18651"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
