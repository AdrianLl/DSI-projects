{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import cPickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = cPickle.load(open('assets/df_postZIP.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>blurb</th>\n",
       "      <th>currency</th>\n",
       "      <th>photos</th>\n",
       "      <th>add_info</th>\n",
       "      <th>num_words</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>wine</th>\n",
       "      <th>outside_dining</th>\n",
       "      <th>parking</th>\n",
       "      <th>wheelchair</th>\n",
       "      <th>private_dining</th>\n",
       "      <th>valet</th>\n",
       "      <th>osm_type</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>new_cuisine</th>\n",
       "      <th>adj_standing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe &amp; MissesDoe</td>\n",
       "      <td>Contemporary</td>\n",
       "      <td>Chef Joe and Jill Doe have revamped their earn...</td>\n",
       "      <td>USD</td>\n",
       "      <td>[]</td>\n",
       "      <td>[$25 to $50, Brunch, Address 45 E. 1st St., Ne...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>40.72379</td>\n",
       "      <td>-73.98969</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>way</td>\n",
       "      <td>10003</td>\n",
       "      <td>contemporary</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              name       cuisine  \\\n",
       "0  Joe & MissesDoe  Contemporary   \n",
       "\n",
       "                                               blurb currency photos  \\\n",
       "0  Chef Joe and Jill Doe have revamped their earn...      USD     []   \n",
       "\n",
       "                                            add_info  num_words  photo_count  \\\n",
       "0  [$25 to $50, Brunch, Address 45 E. 1st St., Ne...        120            0   \n",
       "\n",
       "   latitude  longitude      ...       wine  outside_dining  parking  \\\n",
       "0  40.72379  -73.98969      ...          0               0        0   \n",
       "\n",
       "   wheelchair  private_dining  valet  osm_type    ZIP   new_cuisine  \\\n",
       "0           0               0      0       way  10003  contemporary   \n",
       "\n",
       "   adj_standing  \n",
       "0            12  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(spam.lower().split()))\n",
    "print('\\n')\n",
    "print(Counter(ham.lower().split()))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit([spam])\n",
    "\n",
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(15).transpose()\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "\n",
    "h = HashingVectorizer(binary=True, decode_error='ignore')\n",
    "\n",
    "h_spam = h.fit([spam])\n",
    "h_ham = h.fit([ham])\n",
    "\n",
    "print(h_spam)\n",
    "print('\\n')\n",
    "print(h_ham)\n",
    "\n",
    "df_spam  = pd.DataFrame(h.transform([spam]).todense())\n",
    "\n",
    "df_spam.transpose().sort_values(0, ascending=False).head(15).transpose()\n",
    "df_spam\n",
    "\n",
    "df_ham  = pd.DataFrame(h.transform([ham]).todense())\n",
    "\n",
    "df_ham.transpose().sort_values(0, ascending=False).head(15).transpose()\n",
    "df_ham\n",
    "\n",
    "df = pd.concat([df_spam, df_ham], axis = 0)\n",
    "\n",
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "    \n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)\n",
    "\n",
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('Swimmed'))\n",
    "print(stemmer.stem('Swimming'))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "sentence = \"this is a foo bar sentence\"\n",
    "print([i for i in sentence.split() if i not in stop])\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pos_tag(tok.tokenize(\"today is a great day to learn nlp\"))\n",
    "\n",
    "toke_spam = pos_tag(tok.tokenize(spam))\n",
    "type(pos_tag(tok.tokenize(spam)))\n",
    "\n",
    "toke_ham = pos_tag(tok.tokenize(ham))\n",
    "type(pos_tag(tok.tokenize(ham)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "tf.fit([spam])\n",
    "df_tf  = pd.DataFrame(tf.transform([spam]).todense(),\n",
    "             columns=tf.get_feature_names())\n",
    "\n",
    "df_tf.transpose().sort_values(0, ascending=False).head(15).transpose()\n",
    "\n",
    "\n",
    "tf.fit([ham])\n",
    "df_tf2  = pd.DataFrame(tf.transform([ham]).todense(),\n",
    "             columns=tf.get_feature_names())\n",
    "\n",
    "df_tf2.transpose().sort_values(0, ascending=False).head(15).transpose()\n",
    "\n",
    "df = pd.concat([df_tf, df_tf2])\n",
    "df.index = ['spam', 'ham']\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the movie review list\n",
    "\n",
    "    print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "    for i in range(len(df['blurb'])):\n",
    "        clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
    "\n",
    "\n",
    "    # ****** Create a bag of words from the training set\n",
    "    #\n",
    "    print \"Creating the bag of words...\\n\"\n",
    "\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    # ******* Train a random forest using the bag of words\n",
    "    #\n",
    "    print \"Training the random forest (this may take a while)...\"\n",
    "\n",
    "\n",
    "    # Initialize a Random Forest classifier with 100 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as\n",
    "    # features and the sentiment labels as the response variable\n",
    "    #\n",
    "    # This may take a few minutes to run\n",
    "    forest = forest.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "\n",
    "\n",
    "    # Create an empty list and append the clean reviews one by one\n",
    "    clean_test_reviews = []\n",
    "\n",
    "    print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "    for i in xrange(0,len(test[\"review\"])):\n",
    "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
    "\n",
    "    # Get a bag of words for the test set, and convert to a numpy array\n",
    "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "    test_data_features = test_data_features.toarray()\n",
    "\n",
    "    # Use the random forest to make sentiment label predictions\n",
    "    print \"Predicting test labels...\\n\"\n",
    "    result = forest.predict(test_data_features)\n",
    "\n",
    "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
    "    # a \"sentiment\" column\n",
    "    output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(os.path.join(os.path.dirname(__file__), 'data', 'Bag_of_Words_model.csv'), index=False, quoting=3)\n",
    "    print \"Wrote results to Bag_of_Words_model.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiSentence = 'This restaurant is a step above the rest with its well-balanced menu and \\\n",
    "ability to create innovative versions of traditional recipes. This is thanks to a critical and \\\n",
    "non-nostalgic approach and a focus on light cuisine. The Osteria Francescana has consolidated \\\n",
    "its position as one of Italy\\'s leading restaurants, with a talented chef whose reputation is \\\n",
    "truly international.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()\n",
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "parsedData = parser(unicode(multiSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 956 lol\n",
      "lowercased: 956 lol\n",
      "lemma: 956 lol\n",
      "shape: 28983 xxx\n",
      "prefix: 10443 l\n",
      "suffix: 956 lol\n",
      "log probability: -8.62125778198\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 475 that\n",
      "lowercased: 475 that\n",
      "lemma: 475 that\n",
      "shape: 53740 xxxx\n",
      "prefix: 3598 t\n",
      "suffix: 2768 hat\n",
      "log probability: -4.46450471878\n",
      "Brown cluster id: 84\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.45774888992\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "lol that is rly funny :)\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "This is gr8\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "i rate it 8/8!!!\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "lol NOUN\n",
      "that ADJ\n",
      "is VERB\n",
      "rly ADV\n",
      "funny ADJ\n",
      ":) PUNCT\n",
      "The det boy [] []\n",
      "\n",
      "\n",
      "boy nsubj ran [u'The'] [u'with']\n",
      "\n",
      "\n",
      "with prep boy [] [u'dog']\n",
      "\n",
      "\n",
      "the det dog [] []\n",
      "\n",
      "\n",
      "spotted amod dog [] []\n",
      "\n",
      "\n",
      "dog pobj with [u'the', u'spotted'] []\n",
      "\n",
      "\n",
      "quickly advmod ran [] []\n",
      "\n",
      "\n",
      "ran ROOT ran [u'boy', u'quickly'] [u'after', u'.']\n",
      "\n",
      "\n",
      "after prep ran [] [u'firetruck']\n",
      "\n",
      "\n",
      "the det firetruck [] []\n",
      "\n",
      "\n",
      "firetruck pobj after [u'the'] []\n",
      "\n",
      "\n",
      ". punct ran [] []\n",
      "\n",
      "\n",
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n",
      "349 ORG Apple\n",
      "346 PERSON Steve Jobs\n",
      "356 DATE October\n",
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON it\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(parsedData):\n",
    "    print\"original:\", token.orth, token.orth_\n",
    "    print\"lowercased:\", token.lower, token.lower_\n",
    "    print\"lemma:\", token.lemma, token.lemma_\n",
    "    print\"shape:\", token.shape, token.shape_\n",
    "    print\"prefix:\", token.prefix, token.prefix_\n",
    "    print\"suffix:\", token.suffix, token.suffix_\n",
    "    print\"log probability:\", token.prob\n",
    "    print\"Brown cluster id:\", token.cluster\n",
    "    print\"----------------------------------------\"\n",
    "    if i > 1:\n",
    "        break\n",
    "        \n",
    "# Let's look at the sentences\n",
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print sentence\n",
    "    print '-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-'\n",
    "    \n",
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print token.orth_, token.pos_\n",
    "    \n",
    "# Let's look at the dependencies of this example:\n",
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(unicode(example))\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], \\\n",
    "          [t.orth_ for t in token.rights]\n",
    "    print '\\n'\n",
    "    \n",
    "# Let's look at the named entities of this example:\n",
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(unicode(example))\n",
    "for token in parsedEx:\n",
    "    print token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\"\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print entity.label, entity.label_, ' '.join(t.orth_ for t in entity)\n",
    "    \n",
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser(unicode(messyData))\n",
    "for token in parsedData:\n",
    "    print token.orth_, token.pos_, token.lemma_\n",
    "    \n",
    "# it does pretty well! Note that it does fail on the token \"gr8\", \n",
    "# taking it as a verb rather than an adjective meaning \"great\"\n",
    "# and \"lol\" probably isn't a noun...it's more like an interjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-120297b69466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# you can access known words from the parser's vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NASA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.__getitem__ (spacy/vocab.cpp:6216)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_or_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_or_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab['NASA']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, nasa.repvec))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:10]:   \n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-afe223e46a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's see if it can figure out this analogy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Man is to King as Woman is to ??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwoman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.__getitem__ (spacy/vocab.cpp:6216)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_or_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_or_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab['king']\n",
    "man = parser.vocab['man']\n",
    "woman = parser.vocab['woman']\n",
    "\n",
    "result = king.repvec - man.repvec + woman.repvec\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named subject_object_extraction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5c13d83bc994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_object_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindSVOs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# can still work even without punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"he and his brother shot me and my sister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfindSVOs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named subject_object_extraction"
     ]
    }
   ],
   "source": [
    "from subject_object_extraction import findSVOs\n",
    "\n",
    "# can still work even without punctuation\n",
    "parse = parser(\"he and his brother shot me and my sister\")\n",
    "print(findSVOs(parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "results:\n",
      "('i h8 riting comprehensibly #skoolsux', ':', 'twitter')\n",
      "('planets and stars and rockets and stuff', ':', 'space')\n",
      "('accuracy:', 1.0)\n",
      "----------------------------------------------------------------------------------------------\n",
      "Top 10 features used to predict: \n",
      "Class 1 best: \n",
      "(-0.53174515364897312, u'planet')\n",
      "(-0.35387714355466582, u'space')\n",
      "(-0.21950271095320154, u'mar')\n",
      "(-0.21950271095320154, u'red')\n",
      "(-0.15678781478414799, u'earth')\n",
      "(-0.15678781478414799, u'launch')\n",
      "(-0.15678781478414799, u'rocket')\n",
      "(-0.14909820280997324, u'great')\n",
      "(-0.14909820280997324, u'love')\n",
      "(-0.099773889976904243, u'blue')\n",
      "Class 2 best: \n",
      "(0.40866468414933055, u'twitter')\n",
      "(0.35268364630295895, u'@mention')\n",
      "(0.22672506206603951, u'lol')\n",
      "(0.22672506206603951, u'gr8')\n",
      "(0.20433254740639992, u'social')\n",
      "(0.20433254740639992, u'medium')\n",
      "(0.2043321367429306, u'reddit')\n",
      "(0.2043321367429306, u'fun')\n",
      "(0.12595858423691936, u'window')\n",
      "(0.12595858423691936, u'u')\n",
      "----------------------------------------------------------------------------------------------\n",
      "The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\n",
      "Sample 0: (u'love', 1)(u'space', 2)(u'great', 1)\n",
      "Sample 1: (u'space', 1)(u'planet', 1)(u'cool', 1)(u'glad', 1)(u'exist', 1)\n",
      "Sample 2: (u'lol', 1)(u'@mention', 1)(u'gr8', 1)\n",
      "Sample 3: (u'twitter', 1)(u'reddit', 1)(u'fun', 1)\n",
      "Sample 4: (u'planet', 1)(u'mar', 1)(u'red', 1)\n",
      "Sample 5: (u'@mention', 1)(u'u', 1)(u'skip', 1)(u'window', 1)(u'9', 1)\n",
      "Sample 6: (u'planet', 1)(u'rocket', 1)(u'launch', 1)(u'earth', 1)\n",
      "Sample 7: (u'twitter', 1)(u'social', 1)(u'medium', 1)\n",
      "Sample 8: (u'@mention', 3)(u'hashtag', 1)\n",
      "Sample 9: (u'planet', 1)(u'orbit', 1)(u'sun', 1)(u'little', 1)(u'blue', 1)(u'green', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rebecca/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:66: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "# Every step in a pipeline needs to be a \"transformer\". \n",
    "# Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    \n",
    "# the vectorizer and classifer to use\n",
    "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "# the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# data\n",
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \n",
    "        \"lol @twitterdude that is gr8\", \"twitter &amp; reddit are fun.\", \n",
    "        \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \n",
    "        \"Rockets launch from Earth and go to other planets.\", \"twitter social media &gt; &lt;\", \n",
    "        \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]\n",
    "\n",
    "# train\n",
    "pipe.fit(train, labelsTrain)\n",
    "\n",
    "# test\n",
    "preds = pipe.predict(test)\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"results:\")\n",
    "for (sample, pred) in zip(test, preds):\n",
    "    print(sample, \":\", pred)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"Top 10 features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
    "# let's see what the pipeline was transforming the data into\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train, labelsTrain)\n",
    "\n",
    "# get the features that the vectorizer learned (its vocabulary)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
    "for i in range(len(train)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))\n",
    "    print(\"Sample {}: {}\".format(i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
