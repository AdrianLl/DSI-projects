{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import re\n",
    "import cPickle\n",
    "import time\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start = datetime.datetime.now()\n",
    "# print 'start time: %s\\n' %start\n",
    "\n",
    "# URL = 'http://www.npiamerica.org/research/'\n",
    "# extensions = []\n",
    "# pre_NPI = requests.get(URL)\n",
    "# pre_soup = BeautifulSoup(pre_NPI.content, 'lxml')\n",
    "# for article in pre_soup.find_all('h1', class_='entry-title'):\n",
    "#     try:\n",
    "#         for title in article.find_all('a'):\n",
    "#             extension = title['href']\n",
    "#             extensions.append(extension)\n",
    "#     except:\n",
    "#         extensions.append(None)\n",
    "\n",
    "# print '\\ndone!'\n",
    "# extensions = pd.DataFrame(extensions)\n",
    "# extensions.columns = ['extension']\n",
    "# extensions.to_csv('assets/NPIextensions.csv', index = False, \\\n",
    "#  encoding='utf-8')\n",
    "# finish = datetime.datetime.now()\n",
    "# print '\\nfinish time: %s' %finish\n",
    "# print '\\ntime elapsed: %s' %(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extensions = pd.read_csv('assets/NPIextensions.csv')\n",
    "# URL = 'http://www.npiamerica.org'\n",
    "# article_soup = []\n",
    "# count = 0\n",
    "# for i in range(len(extensions['extension'])): \n",
    "#     time.sleep(30)\n",
    "#     tiny_request = requests.get(URL + extensions['extension'][i])\n",
    "#     if str(tiny_request) != '<Response [200]>':\n",
    "#         print '\\n'\n",
    "#         print URL + extensions['extension'][i]\n",
    "#         count += 1\n",
    "#         print '\\n'\n",
    "#         article_soup.append(None)\n",
    "#     else:\n",
    "#         tiny_soup = BeautifulSoup(tiny_request.content, \"lxml\")\n",
    "#         article_soup.append(tiny_soup)\n",
    "#         print count,\n",
    "#         count += 1\n",
    "\n",
    "# article_soup = pd.DataFrame(article_soup)\n",
    "# article_soup.to_csv('assets/NPI_soup1.csv', index = False, \\\n",
    "#  encoding='utf-8')\n",
    "\n",
    "# finish = datetime.datetime.now()\n",
    "# print '\\ntime elapsed: %s' %(finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NPI_soup = pd.read_csv('assets/NPI_soup1.csv')\n",
    "# NPI_soup = NPI_soup['0'].apply(lambda x: BeautifulSoup(x, 'lxml'))\n",
    "# NPI_soup = NPI_soup.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def articlify(list_soup):\n",
    "    \n",
    "#     title = []\n",
    "#     latitude = []\n",
    "#     longitude = []\n",
    "#     URL = []\n",
    "#     image = []\n",
    "#     articles = []\n",
    "#     author = []\n",
    "#     date = []\n",
    "   \n",
    "#     for tiny_soup in list_soup:\n",
    "        \n",
    "#         ## GET ARTICLE TITLE\n",
    "#         title_result = tiny_soup.findAll('title')\n",
    "#         try:\n",
    "#             for lil_title in title_result:\n",
    "#                 title_text = lil_title.text.encode('utf-8').split(' — ')[0]\n",
    "#                 title.append(title_text)\n",
    "#         except:\n",
    "#             title.append(None)\n",
    "            \n",
    "#         ## GET LATITUDE\n",
    "#         latitude_result = tiny_soup.findAll('meta', property = \"og:latitude\")\n",
    "#         try:\n",
    "#             for lil_lat in latitude_result:\n",
    "#                 latitude_text = lil_lat['content']\n",
    "#                 latitude.append(latitude_text)\n",
    "#         except:\n",
    "#             latitude.append(None)\n",
    "              \n",
    "#         ## GET LONGITUDE\n",
    "#         longitude_result = tiny_soup.findAll('meta', property = \"og:longitude\")\n",
    "#         try:\n",
    "#             for lil_long in longitude_result:\n",
    "#                 longitude_text = lil_long['content']\n",
    "#                 longitude.append(longitude_text)\n",
    "#         except:\n",
    "#             longitude.append(None)\n",
    "            \n",
    "#         ## GET LONGITUDE\n",
    "#         URL_result = tiny_soup.findAll('meta', property = \"og:url\")\n",
    "#         try:\n",
    "#             for lil_URL in URL_result:\n",
    "#                 URL_text = lil_URL['content']\n",
    "#                 URL.append(URL_text)\n",
    "#         except:\n",
    "#             URL.append(None)\n",
    "\n",
    "#         ## GET IMAGE\n",
    "#         image_result = tiny_soup.findAll('meta', property = \"og:image\")\n",
    "#         try:\n",
    "#             for lil_image in image_result:\n",
    "#                 image_text = lil_image['content']\n",
    "#                 image.append(image_text)\n",
    "#         except:\n",
    "#             image.append(None)\n",
    "            \n",
    "#         ## GET ARTICLE\n",
    "#         article_result = tiny_soup.findAll('div', class_ = \"sqs-block code-block \\\n",
    "#                                                                         sqs-block-code\")\n",
    "#         try:\n",
    "#             article_listlet = []\n",
    "#             for lil_article in article_result:\n",
    "#                 article_text = lil_article.text.encode('utf-8')\n",
    "#                 article_listlet.append(article_text)\n",
    "#             articles.append(article_listlet)\n",
    "#         except:\n",
    "#             articles.append(None)\n",
    "            \n",
    "#         ## GET AUTHOR\n",
    "#         author_result = tiny_soup.find('span', class_ = \"author\")\n",
    "#         try:\n",
    "#             for lil_author in author_result:\n",
    "#                 author_text = lil_author.text.encode('utf-8')\n",
    "#                 author.append(author_text)\n",
    "#         except:\n",
    "#             author.append(None)\n",
    "            \n",
    "#         ## GET DATE\n",
    "#         date_result = tiny_soup.find('span', class_ = \"date\")\n",
    "#         try:\n",
    "#             for lil_date in date_result:\n",
    "#                 date_text = lil_date.text.encode('utf-8')\n",
    "#                 date.append(date_text)\n",
    "#         except:\n",
    "#             date.append(None)\n",
    "    \n",
    "#     df = pd.DataFrame(zip(title, latitude, longitude, URL, image, articles, author, date), \\\n",
    "#             columns = ['title', 'latitude', 'longitude', 'URL', 'image', 'articles', \\\n",
    "#                        'author', 'date'])\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = articlify(NPI_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(df['articles'])):\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].replace('.”', '”.')\n",
    "#     #df['articles'][i] = df['articles'][i].replace('[', ' ')  #[9]  9.\n",
    "#     #df['articles'][i] = df['articles'][i].replace(']', ' ')\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].replace('_', '')\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].replace('…', '')\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].replace('Ph.D.', 'PhD')\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv('assets/NPIarticles.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets/NPIarticles.csv', encoding = 'utf-8')\n",
    "# for i in range(len(df['articles'])):\n",
    "#     df.loc[i, 'articles'] = df.loc[i, 'articles'].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(7,9):\n",
    "#     article_result = NPI_soup[i].findAll('div', class_ = \\\n",
    "#                                              \"sqs-block markdown-block sqs-block-markdown\")\n",
    "#     try:\n",
    "#         article_listlet = []\n",
    "#         for lil_article in article_result:\n",
    "#             article_text = lil_article.text.encode('utf-8')\n",
    "#             article_listlet.append(article_text)\n",
    "#         df['articles'][i] = article_listlet\n",
    "#     except:\n",
    "#         df['articles'][i].append(i) \n",
    "        \n",
    "# article_result = NPI_soup[9].findAll('div', class_ = \"sqs-block code-block sqs-block-code\")\n",
    "# try:\n",
    "#     article_listlet = []\n",
    "#     for lil_article in article_result:\n",
    "#         article_text = lil_article.text.encode('utf-8')\n",
    "#         article_listlet.append(article_text)\n",
    "#     df['articles'][9] = article_listlet\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_text = df.loc[0, 'articles']\n",
    "\n",
    "sample_text = df.loc[1, 'articles']\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r'''Chunk: {<RB.?>*<VB.?>*<NN>?}'''\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            \n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df['articles'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
