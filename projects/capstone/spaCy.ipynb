{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiSentence = 'This restaurant is a step above the rest with its well-balanced menu and \\\n",
    "ability to create innovative versions of traditional recipes. This is thanks to a critical and \\\n",
    "non-nostalgic approach and a focus on light cuisine. The Osteria Francescana has consolidated \\\n",
    "its position as one of Italy\\'s leading restaurants, with a talented chef whose reputation is \\\n",
    "truly international.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()\n",
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "parsedData = parser(unicode(multiSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 956 lol\n",
      "lowercased: 956 lol\n",
      "lemma: 956 lol\n",
      "shape: 28983 xxx\n",
      "prefix: 10443 l\n",
      "suffix: 956 lol\n",
      "log probability: -8.62125778198\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 475 that\n",
      "lowercased: 475 that\n",
      "lemma: 475 that\n",
      "shape: 53740 xxxx\n",
      "prefix: 3598 t\n",
      "suffix: 2768 hat\n",
      "log probability: -4.46450471878\n",
      "Brown cluster id: 84\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.45774888992\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "lol that is rly funny :)\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "This is gr8\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "i rate it 8/8!!!\n",
      "-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-\n",
      "lol NOUN\n",
      "that ADJ\n",
      "is VERB\n",
      "rly ADV\n",
      "funny ADJ\n",
      ":) PUNCT\n",
      "The det boy [] []\n",
      "\n",
      "\n",
      "boy nsubj ran [u'The'] [u'with']\n",
      "\n",
      "\n",
      "with prep boy [] [u'dog']\n",
      "\n",
      "\n",
      "the det dog [] []\n",
      "\n",
      "\n",
      "spotted amod dog [] []\n",
      "\n",
      "\n",
      "dog pobj with [u'the', u'spotted'] []\n",
      "\n",
      "\n",
      "quickly advmod ran [] []\n",
      "\n",
      "\n",
      "ran ROOT ran [u'boy', u'quickly'] [u'after', u'.']\n",
      "\n",
      "\n",
      "after prep ran [] [u'firetruck']\n",
      "\n",
      "\n",
      "the det firetruck [] []\n",
      "\n",
      "\n",
      "firetruck pobj after [u'the'] []\n",
      "\n",
      "\n",
      ". punct ran [] []\n",
      "\n",
      "\n",
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n",
      "349 ORG Apple\n",
      "346 PERSON Steve Jobs\n",
      "356 DATE October\n",
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON it\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(parsedData):\n",
    "    print\"original:\", token.orth, token.orth_\n",
    "    print\"lowercased:\", token.lower, token.lower_\n",
    "    print\"lemma:\", token.lemma, token.lemma_\n",
    "    print\"shape:\", token.shape, token.shape_\n",
    "    print\"prefix:\", token.prefix, token.prefix_\n",
    "    print\"suffix:\", token.suffix, token.suffix_\n",
    "    print\"log probability:\", token.prob\n",
    "    print\"Brown cluster id:\", token.cluster\n",
    "    print\"----------------------------------------\"\n",
    "    if i > 1:\n",
    "        break\n",
    "        \n",
    "# Let's look at the sentences\n",
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print sentence\n",
    "    print '-`-`-`-`-`-`-`-`-`-`-`-`-`-`-`-'\n",
    "    \n",
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print token.orth_, token.pos_\n",
    "    \n",
    "# Let's look at the dependencies of this example:\n",
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(unicode(example))\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], \\\n",
    "          [t.orth_ for t in token.rights]\n",
    "    print '\\n'\n",
    "    \n",
    "# Let's look at the named entities of this example:\n",
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(unicode(example))\n",
    "for token in parsedEx:\n",
    "    print token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\"\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print entity.label, entity.label_, ' '.join(t.orth_ for t in entity)\n",
    "    \n",
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser(unicode(messyData))\n",
    "for token in parsedData:\n",
    "    print token.orth_, token.pos_, token.lemma_\n",
    "    \n",
    "# it does pretty well! Note that it does fail on the token \"gr8\", \n",
    "# taking it as a verb rather than an adjective meaning \"great\"\n",
    "# and \"lol\" probably isn't a noun...it's more like an interjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-120297b69466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# you can access known words from the parser's vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NASA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.__getitem__ (spacy/vocab.cpp:6216)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_or_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_or_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab['NASA']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, nasa.repvec))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:10]:   \n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-afe223e46a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's see if it can figure out this analogy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Man is to King as Woman is to ??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwoman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.__getitem__ (spacy/vocab.cpp:6216)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_or_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0morth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_or_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab['king']\n",
    "man = parser.vocab['man']\n",
    "woman = parser.vocab['woman']\n",
    "\n",
    "result = king.repvec - man.repvec + woman.repvec\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_repvec and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named subject_object_extraction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5c13d83bc994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_object_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindSVOs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# can still work even without punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"he and his brother shot me and my sister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfindSVOs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named subject_object_extraction"
     ]
    }
   ],
   "source": [
    "from subject_object_extraction import findSVOs\n",
    "\n",
    "# can still work even without punctuation\n",
    "parse = parser(\"he and his brother shot me and my sister\")\n",
    "print(findSVOs(parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "results:\n",
      "('i h8 riting comprehensibly #skoolsux', ':', 'twitter')\n",
      "('planets and stars and rockets and stuff', ':', 'space')\n",
      "('accuracy:', 1.0)\n",
      "----------------------------------------------------------------------------------------------\n",
      "Top 10 features used to predict: \n",
      "Class 1 best: \n",
      "(-0.53174515364897312, u'planet')\n",
      "(-0.35387714355466582, u'space')\n",
      "(-0.21950271095320154, u'mar')\n",
      "(-0.21950271095320154, u'red')\n",
      "(-0.15678781478414799, u'earth')\n",
      "(-0.15678781478414799, u'launch')\n",
      "(-0.15678781478414799, u'rocket')\n",
      "(-0.14909820280997324, u'great')\n",
      "(-0.14909820280997324, u'love')\n",
      "(-0.099773889976904243, u'blue')\n",
      "Class 2 best: \n",
      "(0.40866468414933055, u'twitter')\n",
      "(0.35268364630295895, u'@mention')\n",
      "(0.22672506206603951, u'lol')\n",
      "(0.22672506206603951, u'gr8')\n",
      "(0.20433254740639992, u'social')\n",
      "(0.20433254740639992, u'medium')\n",
      "(0.2043321367429306, u'reddit')\n",
      "(0.2043321367429306, u'fun')\n",
      "(0.12595858423691936, u'window')\n",
      "(0.12595858423691936, u'u')\n",
      "----------------------------------------------------------------------------------------------\n",
      "The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\n",
      "Sample 0: (u'love', 1)(u'space', 2)(u'great', 1)\n",
      "Sample 1: (u'space', 1)(u'planet', 1)(u'cool', 1)(u'glad', 1)(u'exist', 1)\n",
      "Sample 2: (u'lol', 1)(u'@mention', 1)(u'gr8', 1)\n",
      "Sample 3: (u'twitter', 1)(u'reddit', 1)(u'fun', 1)\n",
      "Sample 4: (u'planet', 1)(u'mar', 1)(u'red', 1)\n",
      "Sample 5: (u'@mention', 1)(u'u', 1)(u'skip', 1)(u'window', 1)(u'9', 1)\n",
      "Sample 6: (u'planet', 1)(u'rocket', 1)(u'launch', 1)(u'earth', 1)\n",
      "Sample 7: (u'twitter', 1)(u'social', 1)(u'medium', 1)\n",
      "Sample 8: (u'@mention', 3)(u'hashtag', 1)\n",
      "Sample 9: (u'planet', 1)(u'orbit', 1)(u'sun', 1)(u'little', 1)(u'blue', 1)(u'green', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rebecca/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:66: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "# Every step in a pipeline needs to be a \"transformer\". \n",
    "# Define a custom transformer to clean text using spaCy\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert text to cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    \n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    \n",
    "# the vectorizer and classifer to use\n",
    "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "# the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# data\n",
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \n",
    "        \"lol @twitterdude that is gr8\", \"twitter &amp; reddit are fun.\", \n",
    "        \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \n",
    "        \"Rockets launch from Earth and go to other planets.\", \"twitter social media &gt; &lt;\", \n",
    "        \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]\n",
    "\n",
    "# train\n",
    "pipe.fit(train, labelsTrain)\n",
    "\n",
    "# test\n",
    "preds = pipe.predict(test)\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"results:\")\n",
    "for (sample, pred) in zip(test, preds):\n",
    "    print(sample, \":\", pred)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"Top 10 features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
    "# let's see what the pipeline was transforming the data into\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train, labelsTrain)\n",
    "\n",
    "# get the features that the vectorizer learned (its vocabulary)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
    "for i in range(len(train)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))\n",
    "    print(\"Sample {}: {}\".format(i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import exifread\n",
    "# Open image file for reading (binary mode)\n",
    "f = open('/Users/Rebecca/Desktop/cherries.jpg', 'rb')\n",
    "\n",
    "# Return Exif tags\n",
    "tags = exifread.process_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-03d9790cdfa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopencv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/imutils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvenience\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvenience\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvenience\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rebecca/anaconda/lib/python2.7/site-packages/imutils/convenience.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cv2"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "# USAGE\n",
    "# python detect_blur.py --images images\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import opencv\n",
    "import cv3\n",
    "\n",
    "def variance_of_laplacian(image):\n",
    "\t# compute the Laplacian of the image and then return the focus\n",
    "\t# measure, which is simply the variance of the Laplacian\n",
    "\treturn cv3.Laplacian(image, cv3.CV_64F).var()\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--images\", required=True,\n",
    "\thelp=\"path to input directory of images\")\n",
    "ap.add_argument(\"-t\", \"--threshold\", type=float, default=100.0,\n",
    "\thelp=\"focus measures that fall below this value will be considered 'blurry'\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in paths.list_images(args[\"images\"]):\n",
    "\t# load the image, convert it to grayscale, and compute the\n",
    "\t# focus measure of the image using the Variance of Laplacian\n",
    "\t# method\n",
    "\timage = cv3.imread(imagePath)\n",
    "\tgray = cv3.cvtColor(image, cv3.COLOR_BGR2GRAY)\n",
    "\tfm = variance_of_laplacian(gray)\n",
    "\ttext = \"Not Blurry\"\n",
    "\n",
    "\t# if the focus measure is less than the supplied threshold,\n",
    "\t# then the image should be considered \"blurry\"\n",
    "\tif fm < args[\"threshold\"]:\n",
    "\t\ttext = \"Blurry\"\n",
    "\n",
    "\t# show the image\n",
    "\tcv3.putText(image, \"{}: {:.2f}\".format(text, fm), (10, 30),\n",
    "\t\tcv3.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "\tcv3.imshow(\"Image\", image)\n",
    "\tkey = cv3.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking imutils\n",
      "  Downloading imutils-0.3.6.tar.gz\n",
      "  Running setup.py egg_info for package imutils\n",
      "    \n",
      "Installing collected packages: imutils\n",
      "  Running setup.py install for imutils\n",
      "    changing mode of build/scripts-2.7/range-detector from 644 to 755\n",
      "    \n",
      "    changing mode of /Users/Rebecca/anaconda/bin/range-detector to 755\n",
      "  Could not find .egg-info directory in install record for imutils\n",
      "Successfully installed imutils\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv\n",
      "\u001b[31m  Could not find a version that satisfies the requirement opencv (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for opencv\u001b[0m\n",
      "Collecting cv2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement cv2 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for cv2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv\n",
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/Rebecca/anaconda/lib/python2.7/site-packages/pip/basecommand.py\", line 134, in main\r\n",
      "    status = self.run(options, args)\r\n",
      "  File \"/Users/Rebecca/anaconda/lib/python2.7/site-packages/pip/commands/freeze.py\", line 73, in run\r\n",
      "    req = pip.FrozenRequirement.from_dist(dist, dependency_links, find_tags=find_tags)\r\n",
      "  File \"/Users/Rebecca/anaconda/lib/python2.7/site-packages/pip/__init__.py\", line 194, in from_dist\r\n",
      "    assert len(specs) == 1 and specs[0][0] == '=='\r\n",
      "AssertionError\r\n",
      "\r\n",
      "Storing complete log in /Users/Rebecca/.pip/pip.log\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
