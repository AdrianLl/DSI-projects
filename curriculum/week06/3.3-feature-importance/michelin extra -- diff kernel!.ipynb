{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import great_circle\n",
    "import time\n",
    "import timeit\n",
    "#from textacy import spacy_utils, extract, export, keyterms, texts #,distance <-- Levenshtein problem\n",
    "from IPython.core.display import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = pd.read_csv('/Users/Rebecca/DSI-projects/projects/capstone/assets/michelin_paths.csv')\n",
    "paths = paths['0']\n",
    "paths.reset_index(drop = True)\n",
    "paths.columns = ['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ba5046e6caee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbig_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtiny_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiny_request\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<Response [200]>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run this cell and stop the kernel to get a partial sample of restaurant data\n",
    "URL = 'http://www.viamichelin.com'\n",
    "big_soup = []\n",
    "for i in paths:\n",
    "    time.sleep(1)\n",
    "    tiny_request = requests.get(URL + i)\n",
    "    if str(tiny_request) != '<Response [200]>':\n",
    "        print URL + i\n",
    "    tiny_soup = BeautifulSoup(tiny_request.content, \"lxml\")\n",
    "    big_soup.append(tiny_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restaurantify(soup_list):\n",
    "    \n",
    "    '''This function creates a dataframe with restaurant observations and columns containing relevant information.'''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    name = []\n",
    "    address = []\n",
    "    cuisine = []\n",
    "    blurb = []\n",
    "    price = []\n",
    "    author = []\n",
    "    distinction = []\n",
    "    standing = []\n",
    "    photo = []\n",
    "    add_info = []\n",
    "        \n",
    "    for tiny_soup in soup_list:\n",
    "\n",
    "        \n",
    "        \n",
    "        for rest_address in tiny_soup.findAll('div', class_ = 'datasheet-item'):\n",
    "            address.append(rest_name.text)\n",
    "       \n",
    "        for rest_cuisine in tiny_soup.findAll('div', class_ = 'datasheet-cooking-type'):\n",
    "            cuisine.append(rest_cuisine.text)\n",
    "     \n",
    "        for rest_blurb in tiny_soup.findAll('div', class_ = 'datasheet-description'):\n",
    "            for j in rest_blurb.find_all('blockquote'):\n",
    "                blurb.append(j.text)\n",
    "         \n",
    "        for rest_price in tiny_soup.findAll('div', class_ = 'datasheet-price'):\n",
    "            price.append(rest_price.text)\n",
    "    \n",
    "        for rest_author in tiny_soup.findAll('p', class_ = 'michelin-guide-inspectors'):\n",
    "            author.append(rest_author.text)\n",
    "    \n",
    "        for rest_distinction in tiny_soup.findAll('div', class_ = 'poi-item-stars'):\n",
    "            for j in rest_distinction.find_all('span'):\n",
    "                distinction.append(j['class'])\n",
    "        \n",
    "        for rest_standing_raw in tiny_soup.findAll('span', class_=re.compile('standing')):\n",
    "            rest_standing = rest_standing_raw['class']\n",
    "            standing.append(rest_standing)\n",
    "        \n",
    "        for rest_photo in tiny_soup.findAll('div', class_ = 'datasheet-more-info datasheet-photo clearfx'):\n",
    "            photo.append(rest_photo.text[0:13])\n",
    "            \n",
    "        for rest_add_info in tiny_soup.findAll('div', class_ = 'datasheet-more-info clearfx'):\n",
    "            add_info.append(rest_add_info.text)\n",
    "    print 'blurb: %s' %len(rest_blurb)\n",
    "    print 'cuisine: %s' %len(rest_cuisine)\n",
    "    print 'address: %s' %len(rest_address)\n",
    "    print 'name: %s' %len(name)   \n",
    "    print 'add_info: %s' %len(rest_add_info)\n",
    "    df = pd.DataFrame(zip(name, address, cuisine, blurb, price, author, distinction, standing, photo, add_info),\\\n",
    "                        columns = ['name', 'address', 'cuisine', 'blurb', 'price', 'author', 'distinction', \\\n",
    "                                   'standing', 'photo', 'add_info']) \n",
    "    print 'length of dataframe: %s' %len(df)\n",
    "    return df #, rest_blurb, rest_cuisine, rest_address, name, rest_add_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def name(result):\n",
    "for rest_name in tiny_soup.find_all('div', class_ = 'datasheet-item datasheet-name'):\n",
    "            name.append(rest_name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = 'www.viamichelin.com'\n",
    "\n",
    "max_results_per_city = 1000\n",
    "\n",
    "##### Create empty dataframe with the columns we want; concatenate to it the new one generated for each city\n",
    "df = pd.DataFrame(columns=['title', 'location', 'company', 'salary', 'city', 'state'])\n",
    "\n",
    "for city in set(['New+York,+NY', 'Los+Angeles,+CA', 'Chicago,+IL', 'Houston,+TX', 'Philadelphia,+PA', 'Phoenix,+AZ', \\\n",
    "'San+Antonio,+TX', 'San+Diego,+CA', 'Dallas,+TX', 'San+Jose,+CA', 'Austin,+TX', 'Jacksonville,+FL', 'San+Francisco,+CA',\\\n",
    "'Indianapolis,+IN', 'Columbus,+OH', 'Fort+Worth,+TX', 'Charlotte,+NC', 'Seattle,+WA', 'Denver,+CO', 'El+Paso,+TX', \\\n",
    "'Detroit,+MI', 'Washington,+DC', 'Boston,+MA', 'Memphis,+TN', 'Nashville,+TN']):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        # Grab the results from the request (as above)\n",
    "        URL = url_template[0] + city + url_template[1] + str(start)\n",
    "        indeed = requests.get(URL)\n",
    "        soup = BeautifulSoup(indeed.content, \"lxml\")\n",
    "        # Append to the full set of results and \n",
    "        x = putitalltogether(soup)\n",
    "        nice_city = city.split(',')\n",
    "        state = nice_city[1]\n",
    "        state = state.replace('+', '')\n",
    "        nice_city = nice_city[0]\n",
    "        nice_city = nice_city.replace('+', ' ')\n",
    "        x['city'] = nice_city\n",
    "        x['state'] = state\n",
    "        df = pd.concat([x, df], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
